{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center\">18.335/6.337 Problem Set 2 - SVD and Gram–Schmidt</div>\n",
    "## <div style=\"text-align: center\">Due Fri, Mar/02 at 11:59 pm</div>\n",
    "### <div style=\"text-align: center\">Created by Wonseok Shin</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"text-align: right\">Solved by [your name] in collaboration with [collaborators, if any]</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\Cmat}[2]{\\mathbb{C}^{#1\\times#2}}\\newcommand{\\Cvec}[1]{\\mathbb{C}^{#1}}\\newcommand{\\Rmat}[2]{\\mathbb{R}^{#1\\times#2}}\\newcommand{\\Rvec}[1]{\\mathbb{R}^{#1}}\\newcommand{\\null}{\\mathrm{null}}\\newcommand{\\range}{\\mathrm{range}}\\newcommand{\\rank}{\\mathrm{rank}}\\newcommand{\\nullity}{\\mathrm{nullity}}\\newcommand{\\sign}{\\mathrm{sign}}\\newcommand{\\norm}[1]{\\left\\|#1\\right\\|}\\newcommand{\\abs}[1]{\\left|#1\\right|}\\newcommand{\\epsmach}{\\epsilon_\\mathrm{machine}}\\newcommand{\\log}{\\mathrm{log}}\\newcommand{\\tanh}{\\mathrm{tanh}}\\newcommand{\\l}{\\lambda}\\newcommand{\\d}{\\delta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General instructions\n",
    "- Please archive all files (this Jupyter notebook plus other files if any) into a single ZIP file and upload it to the course website, so that the grader can run the Jupyter notebook in the extracted folder without adding anything.  (This greatly simplifies the grader's job.)\n",
    "- Make sure to submit the correct version of your solution file.  Download your submission from the course website and check if it is indeed the verison you intended to submit.\n",
    "- Some questions may require proofs or explanations, and there are a few options to submit them electronically.\n",
    "    1. Ideally, write down your proofs or explanations in the corresponding sections of this Jupyter notebook.  You can use $\\LaTeX$ commands for mathematical symbols, like $A = \\hat{U} \\Sigma V^*$, $\\sum_{j = 1}^{r} \\sigma_j u_j v_j^*$, and $\\left[\n",
    "\\begin{array}{cc}\n",
    " 1 & 2 \\\\\n",
    " 3 & 4 \\\\\n",
    "\\end{array}\n",
    "\\right]$.\n",
    "    2. If you are not familiar with $\\LaTeX$ commands, you can use any word processors with equation editors.  If you choose to use this option, make sure to convert the word processor file into PDF before submission.\n",
    "    3. If none of the above works for you, you can also submit a scan of your handwritten notes, but your handwriting must be clean and readable.\n",
    "- The scores of bonus problems, if any, will be used to compensate the points deducted in regular problems at the end of the semester.  In other words, \n",
    "$$\n",
    "\\text{your total p-set score} = \\min\\left\\{\\sum_n\\text{your $n$th p-set score $with$ bonus problems}, \\sum_n\\text{perfect $n$th p-set score $without$ bonus problems}\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.  Low-rank approximation via SVD [2 pts]\n",
    "\n",
    "In this problem, we will see the efficiency of the low-rank approximation of a matrix via SVD.  Specifically, we will see that the low-rank approximation of our $2000\\times2000$ test matrix $A$ allows us to calculate $A x$ much faster with only small error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.1.  Choose the rank  [0.5 pt]\n",
    "\n",
    "When the SVD of $A\\in \\mathbb{C}^{m\\times n}$ is $A=\\sum _{j=1}^p \\sigma _j u_j v_j^*$, the low-rank approximation of $A$ is a partial sum $A_k = \\sum _{j=1}^k \\sigma _j u_j v_j^*$, where $1\\le k \\le p=\\min\\{m,n\\}$.  \n",
    "\n",
    "For an accurate-yet-efficient approximation, the correct choice of the rank $k$ is very important.  Typically, we only keep the terms with relatively large $\\sigma_j$'s.  In this problem set, we will keep all the terms with $\\sigma_j$'s greater than or equal to 1% of $\\sigma_1$.\n",
    "\n",
    "Implement `find_k` below that takes a vector of $\\sigma_j$'s and returns $k$ such that $\\sigma_k / \\sigma_1\\ge 0.01$ and $\\sigma_{k+1}/\\sigma_1<0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function find_k(σ)  # σ: vector of σⱼ's\n",
    "\n",
    "    return k\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.2.  Apply $A_k$ to $x$ [0.5 pt]\n",
    "\n",
    "Now, we implement the operation of $A_k$ on $x$.  Complete `apply_Aₖ` below that takes the SVD components of $A$ together with $k$ and $x$, and returns $A_k x$.\n",
    "\n",
    "Hint: when implementing $A B x$ numerically for some matrices $A$ and $B$, it is very important to implement it as $A (B x)$ rather than $(A B) x$.  Why do you think this is the case?  (You don't have to submit the answer to this question.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function apply_Aₖ(U, σ, V, k, x)\n",
    "\n",
    "    return yₖ  # yₖ = Aₖ x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.3.  Measure the error in $A_k x$ [0.5 pt]\n",
    "\n",
    "To test the accuracy of $A_k x$, complete `eval_err` below that evaluates the average error in $A_k x$ over many $x$'s.  The specific requirements are as follows.\n",
    "\n",
    "- Pass $x$'s as the columns of a matrix $X$.\n",
    "- For each $x$, calculate $A x$ and $A_k x$ (using `apply_Aₖ`), and the error $\\left\\|A_k x-A x\\right\\|_2 / \\left\\| A x\\right\\|_2$.\n",
    "- Repeat the error calculation for all columns $x$ of $X$, and calculate the average error.\n",
    "- Return the average error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function eval_err(A, X, U, σ, V, k)\n",
    "\n",
    "    return err  # average error over columns of X\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.4.  Measure the performance of $A_k x$ [0.5 pt]\n",
    "\n",
    "To evaluate the performance of $A_k x$, complete `eval_time` below that evaluates the average time taken for calculating $A x$ and $A_k x$ over many $x$'s.  The specific requirements are as follows.\n",
    "\n",
    "- Pass $x$'s as the columns of a matrix $X$.\n",
    "- For each $x$, time the calculation of $A x$ and $A_k x$ (using `apply_Aₖ`).  Julia's `@elapsed` macro will be handy for timing.\n",
    "- Repeat this time measurement for all columns $x$ of $X$, and calculate the average time taken for $A x$ and $A_k x$ separately.\n",
    "- Return the average times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function eval_t(A, X, U, σ, V, ν)\n",
    "\n",
    "    return t_A, t_Aₖ  # average time taken for A x, average time taken for Aₖ x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification of your implementation\n",
    "\n",
    "Run the following code block to verify your implementation of the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the test matrix and vectors, and perform SVD.\n",
    "using JLD  # if not installed yet, execute Pkg.add(\"JLD\") first\n",
    "\n",
    "@load \"A.jld\"\n",
    "m, n = size(A)\n",
    "\n",
    "# Find the number of dominant singular values.\n",
    "U, σ, V = svd(A)\n",
    "k = find_k(σ)\n",
    "\n",
    "# Measure the accuracy of the low-rank approximation.\n",
    "N = 10  # number of x's to test\n",
    "X = randn(n,N) + im*randn(n,N)  # N random complex vectors\n",
    "err = eval_err(A, X, U, σ, V, k)\n",
    "\n",
    "# Measure the performance of the low-rank approximation.\n",
    "t_A, t_Aₖ = eval_t(A, X, U, σ, V, k)\n",
    "t_ratio = t_A / t_Aₖ\n",
    "\n",
    "# Print the measurement result.\n",
    "println(\"Comparison between A x and Aₖ x\")\n",
    "println(\"------------------------------------\")\n",
    "println(\"A: m = $m, n = $n, k = $k\\n\")\n",
    "\n",
    "@printf \"Average error in Aₖ x = %.3f%%\\n\\n\" 100err\n",
    "\n",
    "@printf \"time for A x: %e seconds\\n\" t_A\n",
    "@printf \"time for Aₖ x: %e seconds\\n\\n\" t_Aₖ\n",
    "\n",
    "@printf \"Aₖ x is %.2f times faster than A x.\\n\" t_ratio\n",
    "println(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.  Intersection between subspaces [4 pts]\n",
    "\n",
    "(Inspired by Exercise 7.4 of T&B)\n",
    "\n",
    "In this problem, we will develop a method to calculate the intersection between two subspaces $\\range(A)$ and $\\range(B)$ for $A \\in \\Cmat{m}{n}$ and $B \\in \\Cmat{m}{p}$.\n",
    "\n",
    "To begin with, here are a few facts you must have learned in linear algebra class, just in case you forgot them.  Below, $X$ and $Y$ are subspaces of $\\Cvec{m}$.\n",
    "\n",
    "- $X^\\bot = \\{z \\in \\Cvec{m} \\mid x^* z = 0 \\text{ for all } x \\in X\\}$.  In other words, $X^\\bot$ is the set of vectors orthogonal to all vectors in $X$.  It is called the [orthogonal complement](https://en.wikipedia.org/wiki/Orthogonal_complement) of $X$.\n",
    "- $(X^\\bot)^\\bot = X$.\n",
    "- $X + Y = \\{x+y \\mid x \\in X, y \\in Y\\}$.  In other words, $X+Y$ is the set of vectors that can be decomposed into the sum of the elements of $X$ and $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) [1 pt]** Show that $\\range(A)^\\bot = \\null(A^*)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Proof.***\n",
    "\n",
    "[Write your proof here.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) [1 pt]** Complete the function `null` below that takes an arbitrary matrix $A$ and returns orthonormal basis vectors of its nullspace.  (Hint: full SVD can be performed by `svd(A, thin=false)` in Julia.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function null(A)\n",
    "    m, n = size(A)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) [1 pt]** For subspaces of $X$ and $Y$ of $\\Cvec{m}$, show $(X + Y)^\\bot = X^\\bot \\cap Y^\\bot$.  (Hint: for a basis $\\{a_1, \\ldots, a_k\\}$ of $X$, $X = \\range(A)$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Proof.***\n",
    "\n",
    "[Write your proof here.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) [1 pt]** Suppose $A \\in \\Cmat{m}{n}$ and $B \\in \\Cmat{m}{p}$ are full-rank and skinny.  Devise and explain a way to find an orthonormal basis of $\\range(A) \\cap \\range(B)$ by using `null` three times.  Implement your method in the function `interspace` below that takes $A$ and $B$ and returns the orthonormal basis of $\\range(A) \\cap \\range(B)$.  Inside `interspace`, do not perform any matrix factorization or orthogonalization outside `null`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution.***\n",
    "\n",
    "[Describe and explain your algorithm here.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function interspace(A, B)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of your implementation\n",
    "\n",
    "The following script uses `interspace` to solve Exercise 7.4 of T&B.  The solution should be close to `v = [0.534673; 0.703132; 0.468754]`.  Note, however, that our `interspace` can handle more general, higher-dimensional cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a₁ = [0.9, 0.3, 0.2]\n",
    "a₂ = [0.5, 0.9, 0.6]\n",
    "\n",
    "b₁ = [0.1, 0.6, 0.1]\n",
    "b₂ = [0.7, 0.4, 0.6]\n",
    "\n",
    "A = [a₁ a₂]\n",
    "B = [b₁ b₂]\n",
    "\n",
    "v = interspace(A, B)\n",
    "\n",
    "println(\"Intersection is along v = $v.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.  QR factorization via the Gram–Schmidt procedure [4 pts]\n",
    "\n",
    "The functions that perform the reduced QR factorization by the Gram–Schmidt procedure, namely `clgs` and `mgs`, show up several times in numerical examples in T&B.  However, Julia does not provide these functions, because the standard method to perform the QR factorization is the Householder triangularization rather than the Gram-Schmidt procedure.  In this problem, we will write `clgs` and `mgs` so that they could be used later in the course.\n",
    "\n",
    "`clgs` and `mgs` implement the classical and modified Gram–Schmidt procedures described in Algs. 7.1 and 8.1 of T&B.  In addition to them, we will also write an additional function, `mgsc`, which performs the modified Gram–Schmidt procedure along the *column* direction.  In terms of code, `mgsc` should look very close to `clgs`, but it must produce the numerically identical result as `mgs`.\n",
    "\n",
    "A few general directions about writing these functions:\n",
    "\n",
    "- For simplicity, consider only skinny or square input matrices ($A \\in \\mathbb{C}^{m\\times n}$ with $m \\ge n$).\n",
    "- Handle rank-deficient $A$ properly.\n",
    "- For reduced memory footprint, implement the functions such that the variable holding the input $A$ is reused to store the output matrix factor $Q$.  (In other words, overwrite the contents of the variable `A` storing the input $A$ with the computed result $Q$.)  Never allocate memory for $Q$ inside the functions.  This way of implementing algorithms is called an [in-place style](https://en.wikipedia.org/wiki/In-place_algorithm).\n",
    "- Similarly, Never allocated memory for $R$ inside the functions.  Instead, create the variable for $R$ outside and pass to the functions as an input argument.  This way, we can allocated memory for $R$ only once and reuse it over several functions calls.  (See the validation code later.)\n",
    "\n",
    "So, our functions will overwrite the contents of the input variables to store the outputs.  Julia's naming convention for functions modifying the contents of input variables is to [append `!` to function names](http://docs.julialang.org/en/stable/manual/style-guide/#append-to-names-of-functions-that-modify-their-arguments) to warn the users about the (unexpected) change of the contents of the input variables.  For example, for a vector `x`, `sort(x)` returns a separate vector containing the sorted vector, whereas `sort!(x)` overwrites `x` with the sorted result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After sort(x): x = [2, 4, 1, 3]\n",
      "After sort!(x): x = [1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "x = [2, 4, 1, 3]\n",
    "y = sort(x)\n",
    "println(\"After sort(x): x = $x\")\n",
    "\n",
    "x = [2, 4, 1, 3]\n",
    "sort!(x)  # overwrite x\n",
    "println(\"After sort!(x): x = $x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once an algorithm is implemented in in-place style, the out-of-place version of the algorithm can be easily implemented in terms of the in-place version.  For example, the out-of-place `sort` (without trailing `!`) can be written easily by using the in-place `sort!` (with trailing `!`) as\n",
    "\n",
    "```julia\n",
    "function sort(x)\n",
    "    y = copy(x)\n",
    "    sort!(y)\n",
    "    return y\n",
    "end\n",
    "```\n",
    "\n",
    "Note that the above code keeps the contents of `x` intact.  In addition to the reduced memory footprint, the fact that the out-of-place version of an algorithm can be easily written in terms of the in-place version is another reason to prefer the in-place style.  \n",
    "\n",
    "Following Julia's naming convention, we will name our in-place functions `clgs!`, `mgs!`, `mgsc!`.  The out-of-place counter parts, `clgs`, `mgs`, `mgsc`,  will be written in terms these in-place functions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CLGS out-of-place\n",
    "function clgs(A)\n",
    "    m, n = size(A)\n",
    "    Q = copy(A)\n",
    "    R = similar(A, (n,n))\n",
    "    clgs!(Q, R)    \n",
    "    return (Q, R)\n",
    "end\n",
    "\n",
    "# MGS out-of-place\n",
    "function mgs(A)\n",
    "    m, n = size(A)\n",
    "    Q = copy(A)\n",
    "    R = similar(A, (n,n))\n",
    "    mgs!(Q, R)\n",
    "    return (Q, R)\n",
    "end\n",
    "\n",
    "# MGSC out-of-place\n",
    "function mgsc(A)\n",
    "    m, n = size(A)\n",
    "    Q = copy(A)\n",
    "    R = similar(A, (n,n))\n",
    "    mgsc!(Q, R)\n",
    "    return (Q, R)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3.1.  CLGS, MGS, and MGSC [3 pts]\n",
    "\n",
    "Complete `clgs!`, `mgs!`, `mgsc!` below that perform the reduced QR factorization using the classical, modified, column-wise modified Gram–Schmidt procedure, respectively.\n",
    "\n",
    "The points for this assignment are distributed as follows:\n",
    "\n",
    "- 0.5 pts for each function handling full-rank $A$ correctly.\n",
    "- Additional 0.5 pts for each function handling rank-deficient $A$ correctly.\n",
    "\n",
    "You are welcome to introduce your own *helper functions* to reduce the amount of code.  Whenever you recognize recurring patterns in your code, it is a good programming practice to define a helper function that handles those recurring portions.  Defining helper functions, however, is not a requirement of this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CLGS in-place\n",
    "function clgs!(A, R)\n",
    "\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "# MGS in-place\n",
    "function mgs!(A, R)\n",
    "    \n",
    "    return nothing\n",
    "end\n",
    "\n",
    "# MGSC in-place\n",
    "function mgsc!(A, R)\n",
    "    \n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of your implementation\n",
    "\n",
    "Use the following code blocks to validate your implementation of the functions.  In the text this script prints, you should get `true` after each question mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For full-rank A\n",
    "m, n = 10, 7\n",
    "A = rand(Complex128, m, n)\n",
    "\n",
    "# Use in-place functions.\n",
    "# Q and R is allocated only once outside the for loop below.\n",
    "Q = similar(A)\n",
    "R = similar(A, n, n)\n",
    "for f_gs! = (clgs!, mgs!, mgsc!)\n",
    "    Q .= A\n",
    "    f_gs!(Q, R)\n",
    "    println(Symbol(f_gs!), \":\")\n",
    "    println(\"QR ≈ A?  $(Q*R ≈ A)\")\n",
    "    println(\"Q'Q ≈ I?  $(Q'Q ≈ eye(n))\")\n",
    "    println()\n",
    "end\n",
    "\n",
    "# Use out-of-place functions.\n",
    "Qm, Rm = mgs(A)\n",
    "Qmc, Rmc = mgsc(A)\n",
    "println(\"Q's identical for MGS and MGSC?  $(Qm == Qmc)\")\n",
    "println(\"R's identical for MGS and MGSC?  $(Rm == Rmc)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For rank-deficient A\n",
    "A = Float64[\n",
    "    1 2 2 1 \n",
    "    1 -1 1 1\n",
    "    1 2 2 -1\n",
    "    1 -1 1 1\n",
    "]\n",
    "m, n = size(A)\n",
    "\n",
    "# Use in-place functions.\n",
    "# Q and R is allocated only once outside the for loop below.\n",
    "Q = similar(A)\n",
    "R = similar(A, n, n)\n",
    "for f_gs! = (clgs!, mgs!, mgsc!)\n",
    "    Q .= A\n",
    "    f_gs!(Q, R)\n",
    "    println(Symbol(f_gs!), \":\")\n",
    "    println(\"QR ≈ A?  $(Q*R ≈ A)\")\n",
    "    println(\"Q'Q ≈ I?  $(Q'Q ≈ eye(n))\")\n",
    "    println()\n",
    "end\n",
    "\n",
    "Qm, Rm = mgs(A)\n",
    "Qmc, Rmc = mgsc(A)\n",
    "println(\"Q's NOT identical for MGS and MGSC?  $(Qm ≠ Qmc)\")\n",
    "println(\"R's NOT identical for MGS and MGSC?  $(Rm ≠ Rmc)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3.2.  $\\hat{Q}$ of CLGS vs. $\\hat{Q}$ of MGS [1 pt]\n",
    "\n",
    "It is known that the columns of $\\hat{Q}$ constructed by MGS are closer to being orthogonal than those constructed by CLGS.  In this assignment we will numerically verify if this is really the case.\n",
    "\n",
    "The quality of orthogonality of the columns of $\\hat{Q}$ can be measured by the error in $\\hat{Q}^* \\hat{Q}$: if it is closer to $I$, the columns of $\\hat{Q}$ are closer to being orthogonal.\n",
    "\n",
    "Complete `compare_gs` below that measures the errors in $\\hat{Q}^* \\hat{Q}$ of CLGS and MGS for $N$ randomly generated test matrices of size $m$-by-$n$.  Use the matrix 1-norm to measure the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function compare_gs(N, m, n)\n",
    " \n",
    "    return err_cl, err_m  # two length-N vectors containing errors in Q'Q for Q by clgs! and mgs!\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of your implementation\n",
    "\n",
    "Use the following code block to plot the measured errors.  You will see that the errors for MGS stay always lower than the errors for CLGS for all matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 200\n",
    "m, n = 50, 30\n",
    "err_cl, err_m = compare_gs(N, m, n)\n",
    "\n",
    "using PyPlot  # if not installed yet, execute Pkg.add(\"PyPlot\") first\n",
    "\n",
    "semilogy(1:N, err_cl, \"r-\", 1:N, err_m, \"b-\")\n",
    "axis([1,N,1e-15,1e-13])\n",
    "title(\"Orthogonality of Q: CLGS vs. MGS for $m×$n Matrices\")\n",
    "xlabel(\"Matrix Number\")\n",
    "ylabel(\"Error in Q* Q\")\n",
    "legend([\"CLGS\", \"MGS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Problem.  Iterative refinement of the Gram–Schmidt procedure [2 pts]\n",
    "\n",
    "The Gram–Schmidt procedure orthogonalizes the $j$th column $a_j$ of $A$ against the already constructed orthonormal vectors $q_1, \\ldots, q_{j-1}$ as\n",
    "$$\n",
    "v_j = a_j - \\sum_{i=1}^{j-1} r_{ij} q_i\n",
    "$$\n",
    "with $r_{ij} = q_i^* a_j$, and constructs a new orthonormal vector $q_j$ by normalizing $v_j$ as\n",
    "$$\n",
    "q_j = \\frac{v_j}{r_{jj}}\n",
    "$$\n",
    "with $r_{jj} = \\norm{v_j}_2$.  However, for rank-deficient $A$, we have $v_j = 0$ for some $j$ and the above normalization step fails with division by zero.  In fact, one of the goals of Prob. 3 was to handle rank-deficient $A$ properly by avoiding such division by zero.\n",
    "\n",
    "However, avoiding division by zero is not sufficient in some cases.  If $A$ is *nearly* rank-deficient, $v_j$ becomes close to a zero vector for some $j$ because $a_j$ and $\\sum_{i=1}^{j-1} r_{ij} q_i$ are close.  Then, due to catastrophic cancellation between $a_j$ and $\\sum_{i=1}^{j-1} r_{ij} q_i$, the resulting $v_j$ becomes inaccurate and less orthogonal to $q_1, \\ldots, q_{j-1}$.  Therefore, the Gram–Schmidt procedure produces $\\hat{Q}$ with nonorthogonal columns for nearly rank-deficient $A$.\n",
    "\n",
    "One way to overcome this problem is iterative refinement.  The main problem is that $v_j$ calculated above is nonorthogonal to $q_1, \\ldots, q_{j-1}$.  This means $v_j$ still contains some components of $q_1, \\ldots, q_{j-1}$.  Iterative refinement extracts these remaining components of $q_1, \\ldots, q_{j-1}$ by orthogonalizing $v_j$ against them once more, and add these additionally extracted coefficients of $q_1, \\ldots, q_{j-1}$ to the previously extracted coefficients.  By repeating this refinement, we can eventually obtain $v_j$ very orthogonal to $q_1, \\ldots, q_{j-1}$.  This refinement procedure can be expressed as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "&r_{ij} = 0 \\text{ for } 1 \\le i \\le j-1\\\\\n",
    "&v_j^{(0)} = a_j\\\\\n",
    "&\\text{for } k = 0, 1, 2, \\ldots, K-1\\\\\n",
    "&\\qquad v_j^{(k+1)} = v_j^{(k)} - \\sum_{i=1}^{j-1} r_{ij}^{(k)} q_i\\\\\n",
    "&\\qquad r_{ij} \\leftarrow r_{ij} + r_{ij}^{(k)}\\text{ for } 1 \\le i \\le j-1\\\\\n",
    "&\\text{end}\\\\\n",
    "&r_{jj} = \\norm{v_j^{(K)}}_2\\\\\n",
    "&q_j = v_j^{(K)} / r_{jj}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This refinement can be implemented in the classical and column-wise modified Gram–Schmidt procedure.  However, it cannot be implemented in the original modified Gram–Schmidt procedure, because the refinement must be performed several times along the column direction in order to generate a new orthogonal vector $q_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment B.1.  CLGS and MGSC with iterative refinement [1 pt]\n",
    "\n",
    "Complete `clgsi!` and `mgsci!` below that implement the above-described iterative refinement in CLGS and MGSC.  \n",
    "\n",
    "In the above algorithm, $K-1$ is the number of refinement steps.  In practice, even $K-1 = 1$ improves orthogonality of $\\hat{Q}$ dramatically for nearly rank-deficient $A$.  However, in order to achieve the ultimate result, perform refinement until $v_j^{(k+1)}$ becomes numerically orthogonal to $q_1, \\ldots, q_{j-1}$, i.e., until $\\abs{q_i^* v_j^{(k+1)}} < \\epsmach \\norm{v_j^{(k+1)}}_2$.  At the same time, to prevent the refinement from running too long, limit the maximum number of refinement steps to $K = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CLGS with iterative refinement, out-of-place\n",
    "function clgsi(A)\n",
    "    m, n = size(A)\n",
    "    Q = copy(A)\n",
    "    R = similar(A, (n,n))\n",
    "    clgsi!(Q, R)    \n",
    "    return (Q, R)\n",
    "end\n",
    "\n",
    "# MGSC with iterative refinement, out-of-place\n",
    "function mgsci(A)\n",
    "    m, n = size(A)\n",
    "    Q = copy(A)\n",
    "    R = similar(A, (n,n))\n",
    "    mgsci!(Q, R)\n",
    "    return (Q, R)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CLGS with iterative refinement, in-place\n",
    "function clgsi!(A, R)\n",
    "    \n",
    "    return nothing\n",
    "end\n",
    "\n",
    "\n",
    "# MGSC with iterative refinement, in-place\n",
    "function mgsci!(A, R)\n",
    "    \n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of your implementation\n",
    "\n",
    "The following code block shows that `clgs!` and `mgs!` fail to generate $\\hat{Q}$ with orthogonal columns for a nearly rank-deficient matrix $A$, whereas `clgsi!` and `mgsi!` succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For nearly rank-deficient A\n",
    "m, n = 10, 6\n",
    "A = zeros(Complex128, m, n)\n",
    "rand!(@view A[:, [1,2,6]])\n",
    "for j = 3:5\n",
    "    A[:,j] .+= rand() .* (@view A[:,1])\n",
    "    A[:,j] .+= rand() .* (@view A[:,2])\n",
    "end\n",
    "\n",
    "# Use in-place functions.\n",
    "# Q and R is allocated only once outside the for loop below.\n",
    "Q = similar(A)\n",
    "R = similar(A, n, n)\n",
    "for f_gs! = (clgs!, clgsi!, mgs!, mgsci!)\n",
    "    Q .= A\n",
    "    f_gs!(Q, R)\n",
    "    println(Symbol(f_gs!), \":\")\n",
    "    println(\"QR ≈ A?  $(Q*R ≈ A)\")\n",
    "    println(\"Q'Q ≈ I?  $(Q'Q ≈ eye(n))\")\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment B.2.  $\\hat{Q}$ of CLGS vs. $\\hat{Q}$ of MGS with iterative refinement [1 pt]\n",
    "\n",
    "In Prob. 3 we wrote `compare_gs` to demonstrate that MGS generates better $\\hat{Q}$ than CLGS.  Is this still going to be true after applying iterative refinement?  \n",
    "\n",
    "To answer this question, complete `compare_gsi` below that measures the errors in $\\hat{Q}^* \\hat{Q}$ of CLGS and MGSC with iterative refinement for $N$ randomly generated test matrices of size $m$-by-$n$.  Use the matrix 1-norm to measure the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function compare_gsi(N, m, n)\n",
    "    \n",
    "    return err_cl, err_m  # two length-N vectors containing errors in Q'Q for Q by clgsi! and mgsi!\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of your implementation\n",
    "\n",
    "Use the following code block to plot the measured errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 200\n",
    "m, n = 50, 30\n",
    "err_cl, err_m = compare_gsi(N, m, n)\n",
    "\n",
    "using PyPlot  # if not installed yet, execute Pkg.add(\"PyPlot\") first\n",
    "\n",
    "semilogy(1:N, err_cl, \"r-\", 1:N, err_m, \"b-\")\n",
    "axis([1,N,1e-15,1e-13])\n",
    "title(\"Orthogonality of Q: CLGSI vs. MGSI for $m×$n Matrices\")\n",
    "xlabel(\"Matrix Number\")\n",
    "ylabel(\"Error in Q* Q\")\n",
    "legend([\"CLGS\", \"MGS\"])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.6.3-pre",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
